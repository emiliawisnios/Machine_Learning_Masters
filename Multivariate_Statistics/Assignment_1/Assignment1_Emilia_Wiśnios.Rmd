---
title: "Assignment 1"
author: "Emilia Wiśnios"
date: '2022-12-03'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(corrplot)
library(caret)
library(ade4)
library(cluster)
library(MASS)
library(ggplot2)
library(resample)
library(AER)
```

## Exercise 1: Principal Component Analysis

The dataset `pendigits.txt` containes data on pen-based handwritten digits. The data were collected from 44 writers, each of whom wrote 250 examples of the digits 0, 1, 2, ..., 9 in a random order. The digits were written inside boxed of 500x500 pixels on a pressure sensitive tablet. Unknown to the writers, the first 10 digits were ignored as writers became familiar with the input device. 

The raw data on each of the $n=10992$ characters consisted of a sequence $(x_t, y_t) : t =1, .., T$ of tablet coordinates of the pen at fixed time intervals of 100 milliseconds, where $(x_t, y_t)$ where integers in the range 0-500. The data where then normalised to make them invariant to translation and scale distortions. The new coordinates had maximum range between 0 and 100. Then 8 regularly spaced measurements $(x_t, y_t)$ were chosen. This gave a total of 16 input variables. Columns 1-16 denote the variables, column 17 is the class code, 0-9. These are the only columns of interest. 

We start with data loading and data selection:

```{r, include=FALSE}
www_pendigits = 'https://www.mimuw.edu.pl/~noble/courses/MultivariateStatistics/data/pendigits.txt'
pendigits_df = read.csv(www_pendigits, sep=" ", header=FALSE)
pendigits_df = pendigits_df[, 1:17]
```

1. Compute the variance of the 16 variables and show that they are very similar. 

```{r}
col_odd <- seq_len(ncol(pendigits_df)) %% 2  
data_col_odd <- pendigits_df[ , col_odd == 1]
data_col_even <- pendigits_df[ , col_odd == 0] 
```

```{r}
vars = colVars(as.matrix(data_col_odd[sapply(data_col_odd, is.numeric)]))
plot(vars)
```
```{r}
vars = colVars(as.matrix(data_col_even[sapply(data_col_even, is.numeric)]))
plot(vars)
```
In the plots above we can see variances for all numbers for xs and ys. For xs some numbers have very similar variance like 1,3,5. For ys 3, 5 and 7 have almost the same variance. 


2. Carry out a PCA using the covariance matrix. 

To carry out PCA using the covariance matrix, we must center the data (but not scale).
```{r}
PCA_cov_method = prcomp(pendigits_df[,-17], center=TRUE, scale=FALSE)
cumsum(PCA_cov_method$sdev^2 /sum(PCA_cov_method$sdev^2))
```

3. How many PCs explain 80% resp. 90% of the total variation in the data?

Using the scores above: 5 components explain 80% of variance, 7 componens explain 90% of variance.

4. Display the first three PCs using pairwise scatterplots.

Now we want to plot 3 first principal components by plotting 3 scatter plots: (PC1 vs. PC2), (PC2 vs. PC3) and (PC1 vs. PC3).

```{r}
qplot(PCA_cov_method$x[,1], PCA_cov_method$x[,2], xlab = "PC1", ylab = "PC2")
```

```{r}
qplot(PCA_cov_method$x[,1], PCA_cov_method$x[,3], xlab = "PC1", ylab = "PC3")
```

```{r}
qplot(PCA_cov_method$x[,2], PCA_cov_method$x[,3], xlab = "PC2", ylab = "PC3")
```

5. Carry out a PCA using the correlation matrix. Is there any substantial difference?

Now we'll perform PCA using correlation matrix:
```{r}
PCA_corr_method = prcomp(pendigits_df[,-17], center=TRUE, scale=TRUE)
cumsum(PCA_corr_method$sdev^2 /sum(PCA_corr_method$sdev^2))
```
In terms of variance explanation, reaching same levels of variance explanation slightly changed (80% with 5 components too, but 90% with 8 components).

```{r}
qplot(PCA_corr_method$x[,1], PCA_corr_method$x[,2], xlab = "PC1", ylab = "PC2")
```

The plot is different from the covariance one (probably due to the scaing of data).

6. Draw the scree plots, for PCS using covariance and for correlation. How many PCs would you use based on this?

```{r}
qplot(c(1:16), PCA_cov_method$sdev^2 /sum(PCA_cov_method$sdev^2)) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("Variance Explained") +
  ggtitle("Scree Plot for Covariance Based PCA") +
  ylim(0, 0.35)
```

```{r}
qplot(c(1:16), PCA_corr_method$sdev^2 /sum(PCA_corr_method$sdev^2)) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("Variance Explained") +
  ggtitle("Scree Plot for Correlation Based PCA") +
  ylim(0, 0.35)
```
For covariance based PCA it’s harder to choose cut-off point, but we would do it for 5 or 6 components. For correlation based one we would do cut-off on 4 components.

7. Is there ill-conditioning in the data matrix? Base your answer on the PCA.

If, in the correlation matrix there are variables that are 100% redundant, then the inverse of the matrix cannot be computed. For example, if a variable is the sum of two other variables selected for the analysis, then the correlation matrix of those variables cannot be inverted, and the factor analysis can basically not be performed. 
Based on the PCA we can say that the datat is ill-conditioned -- one of the variables in the correlation matrix is fully correlated with others.

## Exercise 2: Mantel Randomisation

Consider the data for ozone measurements from thirty two locations in the Los Angeles area, found in the file `ozone.csv` in the course data directory. Performn a Mantel test to see whether the differences between ozone measurements are smaller for stations that are closer together.

```{r}
www_ozone = "https://www.mimuw.edu.pl/~noble/courses/MultivariateStatistics/data/ozone.csv"
ozone_df = read.csv(www_ozone, header=T)
summary(ozone_df)
```
To teast whether the differences in ozone measurements are smaller for stations that are closer together than for stations that are far apart, we have to generate two distance matrices: one containing spatioal distances and one containig distances between measured outcomes at a given points.

```{r}
station.dists <- dist(cbind(ozone_df$Lon, ozone_df$Lat))
ozone.dists <- dist(ozone_df$Av8top)
```

With these matrices we can test correlation with Mantel test. The test consistes of calculating the correlation of the entries in the matrices, then permuting the matrices and calculating the same test statistic under each permutation and comparing the original test staistic to the distribution of test statistics from the permutations to generate a p-value. 

```{r}
mantel.rtest(station.dists, ozone.dists, nrepet = 9999)
```

Based on these results, we can reject the null hypothesis that these two matrices, spatial distance and ozone distance, are unrelatd with $\alpha = 0.05$. The observed correlation suggests that the matrix entries are positively associated. 

## Exercise 3: Clustering

The data in `primate.scapulae.txt` (and `primate.scapulae.xls`) contain indices and angles that are related to scapular shape (shoulder bones of primates), but not functional meaniong. There are 8 variables in the dataset. The first five (AD.BD, AD.CD, EA.CD, Dx.CD, SH.ACR) are indices and the last three (EAD, $\beta, \gamma$) are angles. Of the 105 measurements on each variable, 16 werer taken on *Hylobates* scapulae, 15 on *Pongo* scapulae, 20 on *Pan* scapulae, 14 on *Gorilla* scapulae and 40 on *Homo* scapulae. The angle $\gamma$ was not available for *Homo*.

We start from data loading and reading desired columns:
```{r}
www_primate_scapulae = 'https://www.mimuw.edu.pl/~noble/courses/MultivariateStatistics/data/primate.scapulae.txt'
primate_scapulae_data = read.csv(www_primate_scapulae, sep=" ")
primate_scapulae_data = primate_scapulae_data[, c(2:8, 11)]
```

1. Apply agglomerative and divisive hierarchical methods for clustering the variables using all 5 indices and the 2 angles available for all ithems. Constuct dendrograms with single-linkage, average-linkage ad complete-linkage and Ward-linkage for the methods.

Below there are desired plots:

a. Dendrogram for single-linkage agglomerative clustering:

```{r}
distances = dist(scale(primate_scapulae_data[, -8]))
single_linkage = hclust(distances, method = "single")
avg_linkage = hclust(distances, method = "average")
complete_linkage = hclust(distances, method = "complete")
ward_linkage = hclust(distances, method = "ward.D")

plot(single_linkage, labels = primate_scapulae_data$classdigit, main = "Single-linkage Dendrogram")
```
b. Dendrogram for average-linkage agglomerative clustering:
```{r}
plot(avg_linkage, labels = primate_scapulae_data$classdigit, main = "Average-linkage Dendrogram")
```
c. Dendrogram for complete-linkage agglomerative clustering:
```{r}
plot(complete_linkage, labels = primate_scapulae_data$classdigit, main = "Complete-linkage Dendrogram")
```
d. Dendrogram for Ward-linkage agglomerative clustering:
```{r}
plot(ward_linkage, labels = primate_scapulae_data$classdigit, main = "Ward-linkage Dendrogram")
```

We can generate additional dendrogram with DIANA algorithm, which does not show potential outliers as in the case of single- and average-linkage methods:
```{r}
diana_algo = diana(distances)
plot(as.dendrogram(diana_algo), main = "DIANA Dendrogram")
```

2. Find the five-cluster solutions for these methods, Construct confusion tabless and compute the missclasification rate. Which methos gives the lowest rate? Which gives the highest rate?

We'll show the clustering accuracy for above methods:

a. Single-linkage agglomerative clustering:
```{r}
confusionMatrix(factor(cutree(single_linkage, k=5)), factor(primate_scapulae_data$classdigit))$overall
```
b. Average-linkage agglomerative clustering:
```{r}
confusionMatrix(factor(cutree(avg_linkage, k=5)), factor(primate_scapulae_data$classdigit))$overall
```
c. Complete-linkage agglomerative clustering:
```{r}
confusionMatrix(factor(cutree(complete_linkage, k=5)), factor(primate_scapulae_data$classdigit))$overall
```
d. Ward-linkage agglomerative clustering:
```{r}
confusionMatrix(factor(cutree(ward_linkage, k=5)), factor(primate_scapulae_data$classdigit))$overall
```
e. DIANA:
```{r}
confusionMatrix(factor(cutree(diana_algo, k=5)), factor(primate_scapulae_data$classdigit))$overall
```

The lowest performing method is average-linkage, and the highest - Ward-linkage and DIANA.

## Excercise 4: Doctor Visits Data

Consider the `DoctorVisits` datat in the AER package. Use a Poisson regression for the number of visits. Is the Poisson model satisfactory? If not, where are the problems and what can be done about them? (**Note** Please notr the limitations of diagnostics for count data. For example, if we have $X_1, ...,X_n$) i.i.d. Bernoulli($\frac{1}{2}$), then each observation will be either 0 or 1, so even if we have the 'correct' model and the 'correct' estimate $\hat{p}=\frac{1}{2}$, the 'error sum of squares' will still be $\sum_{j=1}^n (X_j - \frac{1}{2})^2 = {n}{4}$ which is substantial. Hence, for the negative binomial model, a large residual sum of squares does not necessarily imply that the model is bad.)

We start with loading data from AER package. We then want to use Poisson regression.

```{r}
data("DoctorVisits")
poisson_reg = glm(visits ~ ., family="poisson", data=DoctorVisits)
summary(poisson_reg)
```
We can see that several variables are not significant, therefore the model is not satisfactory. To resolve this problem we can use variable elimination procedure -- iterative process of eliminating the most insignificant variable and fitting the model untill all variables are significant. 

```{r}
poisson_reg = glm(visits ~ . - freerepat, family="poisson", data=DoctorVisits)
poisson_reg = glm(visits ~ . - freerepat - private, family="poisson", data=DoctorVisits)
poisson_reg = glm(visits ~ . - freerepat - private - nchronic, family="poisson", data=DoctorVisits)
poisson_reg = glm(visits ~ . - freerepat - private - nchronic - lchronic, family="poisson", data = DoctorVisits)
summary(poisson_reg)
```
After eliminating following variables all remaining variables are independent with respect to significance level equal to 0.01: `freerepat`, `private`, `nchronic`, `lchronic`.

The next step is verifying if model follows Poisson distribution.

```{r}
dispersiontest(poisson_reg)
```
We are specifically interested in equality between mean and variance. To chcek that we've used dispersion test: the null hypothesis is having equidispersion, the alternative -- overdispersion. Obtained p-value evince that we indeed have the overdispersion problem. To solve this we can try fitting negative binomial regression model, which has an additional parameter to account for overdispersion.

```{r}
nb_reg = glm.nb(visits ~ . - freerepat - private - nchronic - lchronic, data=DoctorVisits)
summary(nb_reg)
```
As we can see we got new insignificant variable: `income`, which we should remove.

```{r}
nb_reg = glm.nb(visits ~ . - freerepat - private - nchronic - lchronic - income, data=DoctorVisits)
summary(nb_reg)
```
The last step is to check if changing the model helped with overdispersion problem. For that we will compare the log-likelihoods of both models:

```{r}
logLik(poisson_reg)
logLik(nb_reg)
```
The second model (negative binomial) has higher log-likelihood score, meanin it fits the data better.

