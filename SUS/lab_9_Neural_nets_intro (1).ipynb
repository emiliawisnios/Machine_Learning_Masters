{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **DIVE 2 DEEP LEARNING textbook**\n",
        "\n",
        "Online book : https://d2l.ai/index.html\n",
        "\n",
        "Pytorch 60 mins tutorial: https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
        "\n",
        "Online book: https://www.deeplearningbook.org\n",
        "\n",
        "Online book: http://neuralnetworksanddeeplearning.com"
      ],
      "metadata": {
        "id": "rxJaFhV7Z48T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U d2l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mAVBhzEjZ20k",
        "outputId": "134f1055-56ac-49f2-df03-ed188bcf2f25"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting d2l\n",
            "  Downloading d2l-1.0.3-py3-none-any.whl (111 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter==1.0.0 (from d2l)\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (from d2l) (1.23.5)\n",
            "Collecting matplotlib==3.7.2 (from d2l)\n",
            "  Downloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib-inline==0.1.6 in /usr/local/lib/python3.10/dist-packages (from d2l) (0.1.6)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from d2l) (2.31.0)\n",
            "Collecting pandas==2.0.3 (from d2l)\n",
            "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy==1.10.1 (from d2l)\n",
            "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l) (6.5.5)\n",
            "Collecting qtconsole (from jupyter==1.0.0->d2l)\n",
            "  Downloading qtconsole-5.4.4-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l) (6.5.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l) (5.5.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l) (7.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (9.4.0)\n",
            "Collecting pyparsing<3.1,>=2.3.1 (from matplotlib==3.7.2->d2l)\n",
            "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l) (2.8.2)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.10/dist-packages (from matplotlib-inline==0.1.6->d2l) (5.7.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->d2l) (2023.3.post1)\n",
            "Collecting tzdata>=2022.1 (from pandas==2.0.3->d2l)\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l) (2023.7.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.7.2->d2l) (1.16.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (6.3.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.6.5)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.0.8)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter==1.0.0->d2l) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter==1.0.0->d2l) (2.16.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.4)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (3.1.2)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (5.3.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (2.1.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.8.0)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (5.9.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (1.2.1)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (23.1.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (1.5.7)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (0.17.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l) (1.0.0)\n",
            "Collecting qtpy>=2.4.0 (from qtconsole->jupyter==1.0.0->d2l)\n",
            "  Downloading QtPy-2.4.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.4/93.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l)\n",
            "  Downloading jedi-0.19.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (4.8.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter==1.0.0->d2l) (3.10.0)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (0.2.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (2.18.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (4.19.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter==1.0.0->d2l) (0.2.6)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->jupyter==1.0.0->d2l) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter==1.0.0->d2l) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->d2l) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter==1.0.0->d2l) (0.5.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (0.8.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (0.10.2)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (1.6.2)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l) (1.15.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (1.1.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l) (2.21)\n",
            "Installing collected packages: tzdata, scipy, qtpy, pyparsing, jedi, pandas, matplotlib, qtconsole, jupyter, d2l\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.2\n",
            "    Uninstalling scipy-1.11.2:\n",
            "      Successfully uninstalled scipy-1.11.2\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.1\n",
            "    Uninstalling pyparsing-3.1.1:\n",
            "      Successfully uninstalled pyparsing-3.1.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed d2l-1.0.3 jedi-0.19.0 jupyter-1.0.0 matplotlib-3.7.2 pandas-2.0.3 pyparsing-3.0.9 qtconsole-5.4.4 qtpy-2.4.0 scipy-1.10.1 tzdata-2023.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of Multilayer Perceptrons from Scratch\n"
      ],
      "metadata": {
        "id": "2LQYlknEaBTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "\n",
        "batch_size = 256\n",
        "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqLz_op5Z9Q8",
        "outputId": "a3d7f15d-5192-478a-d0bb-0d5210c5cdab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:02<00:00, 11711272.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 210019.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 3881992.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 13453132.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing Model Parameters\n",
        "\n",
        "Recall that Fashion-MNIST contains 10 classes,\n",
        "and that each image consists of a $28 \\times 28 = 784$\n",
        "grid of grayscale pixel values.\n",
        "Again, we will disregard the spatial structure\n",
        "among the pixels for now,\n",
        "so we can think of this as simply a classification dataset\n",
        "with 784 input features and 10 classes.\n",
        "To begin, we will [**implement an MLP\n",
        "with one hidden layer and 256 hidden units.**]\n",
        "Note that we can regard both of these quantities\n",
        "as hyperparameters.\n",
        "Typically, we choose layer widths in powers of 2,\n",
        "which tend to be computationally efficient because\n",
        "of how memory is allocated and addressed in hardware.\n",
        "\n",
        "Again, we will represent our parameters with several tensors.\n",
        "Note that *for every layer*, we must keep track of\n",
        "one weight matrix and one bias vector.\n",
        "As always, we allocate memory\n",
        "for the gradients of the loss with respect to these parameters."
      ],
      "metadata": {
        "id": "MRh_Ll65aOW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
        "\n",
        "W1 = nn.Parameter(torch.randn(\n",
        "    num_inputs, num_hiddens, requires_grad=True) * 0.01)\n",
        "b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))\n",
        "W2 = nn.Parameter(torch.randn(\n",
        "    num_hiddens, num_outputs, requires_grad=True) * 0.01)\n",
        "b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))\n",
        "\n",
        "params = [W1, b1, W2, b2]"
      ],
      "metadata": {
        "id": "qnFsSQNKaJ6P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activation Function\n",
        "\n",
        "To make sure we know how everything works,\n",
        "we will [**implement the ReLU activation**] ourselves\n",
        "using the maximum function rather than\n",
        "invoking the built-in `relu` function directly."
      ],
      "metadata": {
        "id": "LUBG3oO6aVVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(X):\n",
        "    a = torch.zeros_like(X)\n",
        "    return torch.max(X, a)"
      ],
      "metadata": {
        "id": "d8bj2l4MaR84"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model\n",
        "\n",
        "Because we are disregarding spatial structure,\n",
        "we `reshape` each two-dimensional image into\n",
        "a flat vector of length  `num_inputs`.\n",
        "Finally, we (**implement our model**)\n",
        "with just a few lines of code.\n"
      ],
      "metadata": {
        "id": "CRykaSKqaZ1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def net(X):\n",
        "    X = X.reshape((-1, num_inputs))\n",
        "    H = relu(X@W1 + b1)  # Here '@' stands for matrix multiplication\n",
        "    return (H@W2 + b2)"
      ],
      "metadata": {
        "id": "rpofAsmAaXJU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Function\n"
      ],
      "metadata": {
        "id": "mMHFUvZuafDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss(reduction='none')\n"
      ],
      "metadata": {
        "id": "3MSdsY1Qab0G"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "Fortunately, [**the training loop for MLPs\n",
        "is exactly the same as for softmax regression.**]\n",
        "Leveraging the `d2l` package again,\n",
        "we call the `train_ch3` function\n",
        "(see `sec_softmax_scratch`),\n",
        "setting the number of epochs to 10\n",
        "and the learning rate to 0.1."
      ],
      "metadata": {
        "id": "-Q6mwSmgar89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs, lr = 10, 0.1\n",
        "updater = torch.optim.SGD(params, lr=lr)\n",
        "#d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)"
      ],
      "metadata": {
        "id": "vGyQnCZYan2a"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# d2l.predict_ch3(net, test_iter)"
      ],
      "metadata": {
        "id": "5V3Ufv-vaq5W"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concise Implementation of Multilayer Perceptrons"
      ],
      "metadata": {
        "id": "sDFW-toZa0Su"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "We add *two* fully-connected layers\n",
        "(previously, we added *one*).\n",
        "The first is [**our hidden layer**],\n",
        "which (**contains 256 hidden units\n",
        "and applies the ReLU activation function**).\n",
        "The second is our output layer."
      ],
      "metadata": {
        "id": "67R9yhn1a6SN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = nn.Sequential(nn.Flatten(),\n",
        "                    nn.Linear(784, 256),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(256, 10))\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        nn.init.normal_(m.weight, std=0.01)\n",
        "\n",
        "net.apply(init_weights);"
      ],
      "metadata": {
        "id": "UnRMF5s3a3kK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, lr, num_epochs = 256, 0.1, 10\n",
        "loss = nn.CrossEntropyLoss(reduction='none')\n",
        "trainer = torch.optim.SGD(net.parameters(), lr=lr)\n",
        "\n",
        "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
        "# d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)"
      ],
      "metadata": {
        "id": "yjJxGiUQa805"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises\n",
        "\n",
        "1. Try adding different numbers of hidden layers (you may also modify the learning rate). What setting works best?\n",
        "1. Try out different activation functions. Which one works best?\n",
        "1. Try different schemes for initializing the weights. What method works best?"
      ],
      "metadata": {
        "id": "vYWWaF7_bcws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 0:\n",
        "Read\n",
        "https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py\n",
        "\n",
        "### Task 1. MLP for Binary Classification\n",
        "\n",
        "We will use the Ionosphere binary (two-class) classification dataset to demonstrate an MLP for binary classification.\n",
        "\n",
        "This dataset involves predicting whether a structure is in the atmosphere or not given radar returns.\n",
        "\n",
        "The dataset will be downloaded automatically using [Pandas](https://pandas.pydata.org/), but you can learn more about it here.\n",
        "\n",
        "*  [Ionosphere Dataset (csv)](https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv).\n",
        "* [Ionosphere Dataset Description (csv)](https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.names).\n",
        "\n",
        "We will use a [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) to encode the string labels to integer values 0 and 1. The model will be fit on 67 percent of the data, and the remaining 33 percent will be used for evaluation, split using the [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function.\n",
        "\n",
        "It is a good practice to use ‘relu‘ activation with a ‘he_normal‘ weight initialization. This combination goes a long way to overcome the problem of vanishing gradients when training deep neural network models. For more on ReLU, see the tutorial:\n",
        "[A Gentle Introduction to the Rectified Linear Unit (ReLU)](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/).\n",
        "\n",
        "The model predicts the probability of class 1 and uses the sigmoid activation function. The model is optimized using the adam version of stochastic gradient descent and seeks to minimize the cross-entropy loss.\n",
        "\n",
        "Implement this network in Pytorch\n",
        "\n"
      ],
      "metadata": {
        "id": "QvpxABCkc6Ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mlp for binary classification\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# load the dataset\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\n",
        "df = read_csv(path, header=None)\n",
        "# split into input and output columns\n",
        "X, y = df.values[:, :-1], df.values[:, -1]\n",
        "# ensure all data are floating point values\n",
        "X = X.astype('float32')\n",
        "# encode strings to integer\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "# split into train and test datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PFAFc3nfL2Q",
        "outputId": "76bbc8cd-5082-4f3d-c8af-1fc051117a7d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(235, 34) (116, 34) (235,) (116,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, n_inputs):\n",
        "        super(MLP, self).__init__()\n",
        "        self.hidden1 = nn.Linear(n_inputs, 10)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.hidden2 = nn.Linear(10, 8)\n",
        "        self.act2 = nn.ReLU()\n",
        "        self.hidden3 = nn.Linear(8, 1)\n",
        "        self.act3 = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = self.hidden1(X)\n",
        "        X = self.act1(X)\n",
        "        X = self.hidden2(X)\n",
        "        X = self.act2(X)\n",
        "        X = self.hidden3(X)\n",
        "        X = self.act3(X)\n",
        "        return X\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class Dataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y.reshape(-1, 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return [self.X[idx], self.y[idx]]\n",
        "\n",
        "train_dl = DataLoader(Dataset(X_train, y_train), batch_size=32, shuffle=True)\n",
        "test_dl = DataLoader(Dataset(X_test, y_test), batch_size=1024, shuffle=False)\n",
        "\n",
        "net = MLP(X_train.shape[1])\n",
        "print(net)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "for epoch in range(100):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    eval_loss = 0\n",
        "    eval_acc = 0\n",
        "    for i, (X, y) in enumerate(train_dl):\n",
        "        optimizer.zero_grad()\n",
        "        yhat = net(X)\n",
        "        loss = criterion(yhat, y.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (X, y) in enumerate(test_dl):\n",
        "            yhat = net(X)\n",
        "            loss = criterion(yhat, y.float())\n",
        "            eval_loss += loss.item()\n",
        "            yhat = torch.round(yhat)\n",
        "            acc = (yhat == y).sum().item()\n",
        "            eval_acc += acc\n",
        "\n",
        "    print('epoch: %d, Train loss %.3f, Test loss %.3f Test acc %.3f' % (epoch, epoch_loss, eval_loss, eval_acc/len(y_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQFdUZO5o9Nr",
        "outputId": "77a8bec7-8e37-413d-a2b1-404eb1990623"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (hidden1): Linear(in_features=34, out_features=10, bias=True)\n",
            "  (act1): ReLU()\n",
            "  (hidden2): Linear(in_features=10, out_features=8, bias=True)\n",
            "  (act2): ReLU()\n",
            "  (hidden3): Linear(in_features=8, out_features=1, bias=True)\n",
            "  (act3): Sigmoid()\n",
            ")\n",
            "epoch: 0, Train loss 5.607, Test loss 0.703 Test acc 0.302\n",
            "epoch: 1, Train loss 5.564, Test loss 0.696 Test acc 0.302\n",
            "epoch: 2, Train loss 5.519, Test loss 0.690 Test acc 0.569\n",
            "epoch: 3, Train loss 5.485, Test loss 0.685 Test acc 0.828\n",
            "epoch: 4, Train loss 5.458, Test loss 0.680 Test acc 0.828\n",
            "epoch: 5, Train loss 5.419, Test loss 0.675 Test acc 0.802\n",
            "epoch: 6, Train loss 5.382, Test loss 0.670 Test acc 0.793\n",
            "epoch: 7, Train loss 5.338, Test loss 0.664 Test acc 0.767\n",
            "epoch: 8, Train loss 5.293, Test loss 0.657 Test acc 0.767\n",
            "epoch: 9, Train loss 5.242, Test loss 0.649 Test acc 0.776\n",
            "epoch: 10, Train loss 5.207, Test loss 0.641 Test acc 0.776\n",
            "epoch: 11, Train loss 5.123, Test loss 0.632 Test acc 0.767\n",
            "epoch: 12, Train loss 5.020, Test loss 0.622 Test acc 0.776\n",
            "epoch: 13, Train loss 4.973, Test loss 0.610 Test acc 0.776\n",
            "epoch: 14, Train loss 4.846, Test loss 0.598 Test acc 0.776\n",
            "epoch: 15, Train loss 4.697, Test loss 0.584 Test acc 0.793\n",
            "epoch: 16, Train loss 4.645, Test loss 0.568 Test acc 0.793\n",
            "epoch: 17, Train loss 4.500, Test loss 0.552 Test acc 0.802\n",
            "epoch: 18, Train loss 4.355, Test loss 0.534 Test acc 0.802\n",
            "epoch: 19, Train loss 4.138, Test loss 0.516 Test acc 0.828\n",
            "epoch: 20, Train loss 4.040, Test loss 0.497 Test acc 0.871\n",
            "epoch: 21, Train loss 3.889, Test loss 0.478 Test acc 0.879\n",
            "epoch: 22, Train loss 3.644, Test loss 0.459 Test acc 0.888\n",
            "epoch: 23, Train loss 3.416, Test loss 0.440 Test acc 0.888\n",
            "epoch: 24, Train loss 3.295, Test loss 0.422 Test acc 0.888\n",
            "epoch: 25, Train loss 3.095, Test loss 0.405 Test acc 0.888\n",
            "epoch: 26, Train loss 2.970, Test loss 0.388 Test acc 0.897\n",
            "epoch: 27, Train loss 3.017, Test loss 0.373 Test acc 0.897\n",
            "epoch: 28, Train loss 2.778, Test loss 0.358 Test acc 0.897\n",
            "epoch: 29, Train loss 2.581, Test loss 0.345 Test acc 0.905\n",
            "epoch: 30, Train loss 2.375, Test loss 0.334 Test acc 0.905\n",
            "epoch: 31, Train loss 2.369, Test loss 0.324 Test acc 0.897\n",
            "epoch: 32, Train loss 2.086, Test loss 0.313 Test acc 0.914\n",
            "epoch: 33, Train loss 2.094, Test loss 0.303 Test acc 0.914\n",
            "epoch: 34, Train loss 2.085, Test loss 0.294 Test acc 0.914\n",
            "epoch: 35, Train loss 2.036, Test loss 0.286 Test acc 0.914\n",
            "epoch: 36, Train loss 1.752, Test loss 0.279 Test acc 0.914\n",
            "epoch: 37, Train loss 1.899, Test loss 0.274 Test acc 0.914\n",
            "epoch: 38, Train loss 1.705, Test loss 0.266 Test acc 0.922\n",
            "epoch: 39, Train loss 1.660, Test loss 0.260 Test acc 0.922\n",
            "epoch: 40, Train loss 1.613, Test loss 0.254 Test acc 0.922\n",
            "epoch: 41, Train loss 1.568, Test loss 0.248 Test acc 0.922\n",
            "epoch: 42, Train loss 1.513, Test loss 0.244 Test acc 0.922\n",
            "epoch: 43, Train loss 1.375, Test loss 0.238 Test acc 0.922\n",
            "epoch: 44, Train loss 1.456, Test loss 0.232 Test acc 0.914\n",
            "epoch: 45, Train loss 1.283, Test loss 0.227 Test acc 0.922\n",
            "epoch: 46, Train loss 1.261, Test loss 0.223 Test acc 0.922\n",
            "epoch: 47, Train loss 1.239, Test loss 0.222 Test acc 0.914\n",
            "epoch: 48, Train loss 1.185, Test loss 0.216 Test acc 0.922\n",
            "epoch: 49, Train loss 1.185, Test loss 0.211 Test acc 0.922\n",
            "epoch: 50, Train loss 1.284, Test loss 0.208 Test acc 0.922\n",
            "epoch: 51, Train loss 1.130, Test loss 0.205 Test acc 0.914\n",
            "epoch: 52, Train loss 1.191, Test loss 0.203 Test acc 0.922\n",
            "epoch: 53, Train loss 1.215, Test loss 0.202 Test acc 0.931\n",
            "epoch: 54, Train loss 1.022, Test loss 0.200 Test acc 0.922\n",
            "epoch: 55, Train loss 1.018, Test loss 0.197 Test acc 0.922\n",
            "epoch: 56, Train loss 0.964, Test loss 0.195 Test acc 0.922\n",
            "epoch: 57, Train loss 0.924, Test loss 0.193 Test acc 0.922\n",
            "epoch: 58, Train loss 0.915, Test loss 0.191 Test acc 0.922\n",
            "epoch: 59, Train loss 0.883, Test loss 0.189 Test acc 0.922\n",
            "epoch: 60, Train loss 0.965, Test loss 0.187 Test acc 0.922\n",
            "epoch: 61, Train loss 0.865, Test loss 0.186 Test acc 0.922\n",
            "epoch: 62, Train loss 0.831, Test loss 0.184 Test acc 0.931\n",
            "epoch: 63, Train loss 0.822, Test loss 0.182 Test acc 0.931\n",
            "epoch: 64, Train loss 0.819, Test loss 0.180 Test acc 0.940\n",
            "epoch: 65, Train loss 0.805, Test loss 0.180 Test acc 0.931\n",
            "epoch: 66, Train loss 0.781, Test loss 0.180 Test acc 0.922\n",
            "epoch: 67, Train loss 0.761, Test loss 0.178 Test acc 0.931\n",
            "epoch: 68, Train loss 0.743, Test loss 0.177 Test acc 0.931\n",
            "epoch: 69, Train loss 0.721, Test loss 0.175 Test acc 0.940\n",
            "epoch: 70, Train loss 0.803, Test loss 0.174 Test acc 0.940\n",
            "epoch: 71, Train loss 0.710, Test loss 0.173 Test acc 0.940\n",
            "epoch: 72, Train loss 0.679, Test loss 0.173 Test acc 0.940\n",
            "epoch: 73, Train loss 0.762, Test loss 0.174 Test acc 0.931\n",
            "epoch: 74, Train loss 0.691, Test loss 0.172 Test acc 0.931\n",
            "epoch: 75, Train loss 0.652, Test loss 0.172 Test acc 0.931\n",
            "epoch: 76, Train loss 0.641, Test loss 0.172 Test acc 0.931\n",
            "epoch: 77, Train loss 0.633, Test loss 0.172 Test acc 0.931\n",
            "epoch: 78, Train loss 0.659, Test loss 0.172 Test acc 0.931\n",
            "epoch: 79, Train loss 0.692, Test loss 0.170 Test acc 0.940\n",
            "epoch: 80, Train loss 0.630, Test loss 0.168 Test acc 0.940\n",
            "epoch: 81, Train loss 0.587, Test loss 0.168 Test acc 0.940\n",
            "epoch: 82, Train loss 0.610, Test loss 0.168 Test acc 0.940\n",
            "epoch: 83, Train loss 0.571, Test loss 0.169 Test acc 0.940\n",
            "epoch: 84, Train loss 0.586, Test loss 0.170 Test acc 0.931\n",
            "epoch: 85, Train loss 0.575, Test loss 0.168 Test acc 0.940\n",
            "epoch: 86, Train loss 0.557, Test loss 0.169 Test acc 0.940\n",
            "epoch: 87, Train loss 0.603, Test loss 0.169 Test acc 0.940\n",
            "epoch: 88, Train loss 0.542, Test loss 0.170 Test acc 0.931\n",
            "epoch: 89, Train loss 0.538, Test loss 0.170 Test acc 0.931\n",
            "epoch: 90, Train loss 0.546, Test loss 0.169 Test acc 0.940\n",
            "epoch: 91, Train loss 0.590, Test loss 0.169 Test acc 0.940\n",
            "epoch: 92, Train loss 0.522, Test loss 0.170 Test acc 0.931\n",
            "epoch: 93, Train loss 0.538, Test loss 0.172 Test acc 0.940\n",
            "epoch: 94, Train loss 0.515, Test loss 0.171 Test acc 0.931\n",
            "epoch: 95, Train loss 0.501, Test loss 0.172 Test acc 0.931\n",
            "epoch: 96, Train loss 0.487, Test loss 0.172 Test acc 0.931\n",
            "epoch: 97, Train loss 0.479, Test loss 0.170 Test acc 0.940\n",
            "epoch: 98, Train loss 0.475, Test loss 0.171 Test acc 0.931\n",
            "epoch: 99, Train loss 0.580, Test loss 0.171 Test acc 0.931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2. MLP for Regression\n",
        "\n",
        "We will use the Boston housing regression dataset to demonstrate an MLP for regression predictive modeling.\n",
        "\n",
        "This problem involves predicting house value based on properties of the house and neighborhood.\n",
        "\n",
        "The dataset will be downloaded automatically using Pandas, but you can learn more about it here.\n",
        "\n",
        "* [Boston Housing Dataset (csv).](https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv)\n",
        "* [Boston Housing Dataset Description (csv).](https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.names)\n",
        "\n",
        "This is a regression problem that involves predicting a single numerical value. As such, the output layer has a single node and uses the default or linear activation function (no activation function). The mean squared error (mse) loss is minimized when fitting the model.\n",
        "\n",
        "Recall that this is a regression, not classification; therefore, we cannot calculate classification accuracy. The complete example of fitting and evaluating an MLP on the Boston housing dataset is listed below."
      ],
      "metadata": {
        "id": "Aim0g2yBfEQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mlp for regression\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "# load the dataset\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
        "df = read_csv(path, header=None)"
      ],
      "metadata": {
        "id": "y28hUePTbFdu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "cSpz1B-GfF_0",
        "outputId": "9c354a86-84a0-400f-b7a4-7ce3125b841f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0     1     2   3      4      5     6       7   8      9     10  \\\n",
              "0  0.00632  18.0  2.31   0  0.538  6.575  65.2  4.0900   1  296.0  15.3   \n",
              "1  0.02731   0.0  7.07   0  0.469  6.421  78.9  4.9671   2  242.0  17.8   \n",
              "2  0.02729   0.0  7.07   0  0.469  7.185  61.1  4.9671   2  242.0  17.8   \n",
              "3  0.03237   0.0  2.18   0  0.458  6.998  45.8  6.0622   3  222.0  18.7   \n",
              "4  0.06905   0.0  2.18   0  0.458  7.147  54.2  6.0622   3  222.0  18.7   \n",
              "\n",
              "       11    12    13  \n",
              "0  396.90  4.98  24.0  \n",
              "1  396.90  9.14  21.6  \n",
              "2  392.83  4.03  34.7  \n",
              "3  394.63  2.94  33.4  \n",
              "4  396.90  5.33  36.2  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aa9a9fcd-17ce-4bf3-9c46-bfcc3d93ae58\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aa9a9fcd-17ce-4bf3-9c46-bfcc3d93ae58')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-aa9a9fcd-17ce-4bf3-9c46-bfcc3d93ae58 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-aa9a9fcd-17ce-4bf3-9c46-bfcc3d93ae58');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3a03d8e3-268a-46e1-8981-187469f48411\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3a03d8e3-268a-46e1-8981-187469f48411')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3a03d8e3-268a-46e1-8981-187469f48411 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cU9kz4elqn89",
        "outputId": "a0a261a5-01d1-40db-f0c9-70dd2251beb1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], dtype='int64')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=[0]), df[0], test_size=0.33)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "tByaSu01fHit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f35b66b-3a0a-422d-ed5c-b46762baf4f2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(339, 13) (167, 13) (339,) (167,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "y_train = y_train.reset_index(drop=True)\n",
        "y_test = y_test.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "kGsm8jEprO4J"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mlp for regression with mse loss function and adam optimizer\n",
        "\n",
        "# define the network\n",
        "class MLP(nn.Module):\n",
        "\n",
        "        def __init__(self, n_inputs):\n",
        "            super(MLP, self).__init__()\n",
        "            self.hidden1 = nn.Linear(n_inputs, 10)\n",
        "            self.act1 = nn.ReLU()\n",
        "            self.hidden2 = nn.Linear(10, 8)\n",
        "            self.act2 = nn.ReLU()\n",
        "            self.hidden3 = nn.Linear(8, 1)\n",
        "\n",
        "        def forward(self, X):\n",
        "            X = X.to(torch.float32)\n",
        "            X = self.hidden1(X)\n",
        "            X = self.act1(X)\n",
        "            X = self.hidden2(X)\n",
        "            X = self.act2(X)\n",
        "            X = self.hidden3(X)\n",
        "            return X\n",
        "\n",
        "# train the model\n",
        "\n",
        "#create dataloader\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class Dataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return [self.X.loc[idx].values, self.y.loc[idx]]\n",
        "\n",
        "\n",
        "train_dl = DataLoader(Dataset(X_train, y_train), batch_size=32, shuffle=True)\n",
        "test_dl = DataLoader(Dataset(X_test, y_test), batch_size=1024, shuffle=False)\n",
        "\n",
        "net = MLP(X_train.shape[1])\n",
        "print(net)\n",
        "# define the optimization\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "# enumerate epochs\n",
        "for epoch in range(100):\n",
        "\n",
        "        epoch_loss = 0\n",
        "        eval_loss = 0\n",
        "        # enumerate mini batches\n",
        "        for i, (X, y) in enumerate(train_dl):\n",
        "            # clear the gradients\n",
        "            optimizer.zero_grad()\n",
        "            # compute the model output\n",
        "            yhat = net(X)\n",
        "            # calculate loss\n",
        "            yhat = yhat.squeeze(1)\n",
        "            loss = criterion(yhat, y.float())\n",
        "            # credit assignment\n",
        "            loss.backward()\n",
        "            # update model weights\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # evaluate the model\n",
        "        with torch.no_grad():\n",
        "            # use test_dl\n",
        "            for i, (X, y) in enumerate(test_dl):\n",
        "                # make prediction\n",
        "                yhat = net(X)\n",
        "                yhat = yhat.squeeze(1)\n",
        "                # calculate loss\n",
        "                loss = criterion(yhat, y.float())\n",
        "                # credit assignment\n",
        "                eval_loss += loss.item()\n",
        "\n",
        "        print('epoch: %d, Train loss %.3f, Test loss %.3f' % (epoch, epoch_loss, eval_loss))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMcY1Bq9qsSi",
        "outputId": "587fe7d5-5676-4df8-a1d6-d17be7ccce97"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (hidden1): Linear(in_features=13, out_features=10, bias=True)\n",
            "  (act1): ReLU()\n",
            "  (hidden2): Linear(in_features=10, out_features=8, bias=True)\n",
            "  (act2): ReLU()\n",
            "  (hidden3): Linear(in_features=8, out_features=1, bias=True)\n",
            ")\n",
            "epoch: 0, Train loss 3801.039, Test loss 115.912\n",
            "epoch: 1, Train loss 1857.728, Test loss 46.214\n",
            "epoch: 2, Train loss 1136.745, Test loss 40.455\n",
            "epoch: 3, Train loss 965.694, Test loss 35.257\n",
            "epoch: 4, Train loss 850.126, Test loss 25.493\n",
            "epoch: 5, Train loss 776.548, Test loss 20.477\n",
            "epoch: 6, Train loss 740.352, Test loss 18.300\n",
            "epoch: 7, Train loss 705.084, Test loss 18.102\n",
            "epoch: 8, Train loss 762.705, Test loss 17.494\n",
            "epoch: 9, Train loss 668.393, Test loss 16.810\n",
            "epoch: 10, Train loss 657.679, Test loss 17.225\n",
            "epoch: 11, Train loss 652.680, Test loss 17.351\n",
            "epoch: 12, Train loss 650.931, Test loss 16.605\n",
            "epoch: 13, Train loss 647.722, Test loss 16.232\n",
            "epoch: 14, Train loss 665.205, Test loss 17.190\n",
            "epoch: 15, Train loss 644.973, Test loss 17.483\n",
            "epoch: 16, Train loss 679.842, Test loss 16.537\n",
            "epoch: 17, Train loss 709.649, Test loss 17.316\n",
            "epoch: 18, Train loss 644.449, Test loss 18.314\n",
            "epoch: 19, Train loss 646.560, Test loss 16.988\n",
            "epoch: 20, Train loss 660.699, Test loss 17.681\n",
            "epoch: 21, Train loss 772.614, Test loss 16.345\n",
            "epoch: 22, Train loss 638.461, Test loss 18.738\n",
            "epoch: 23, Train loss 640.098, Test loss 16.462\n",
            "epoch: 24, Train loss 645.436, Test loss 16.341\n",
            "epoch: 25, Train loss 703.667, Test loss 17.541\n",
            "epoch: 26, Train loss 634.878, Test loss 17.352\n",
            "epoch: 27, Train loss 699.121, Test loss 17.219\n",
            "epoch: 28, Train loss 640.139, Test loss 18.034\n",
            "epoch: 29, Train loss 633.369, Test loss 16.344\n",
            "epoch: 30, Train loss 640.074, Test loss 15.688\n",
            "epoch: 31, Train loss 676.008, Test loss 16.719\n",
            "epoch: 32, Train loss 642.010, Test loss 18.564\n",
            "epoch: 33, Train loss 645.013, Test loss 15.957\n",
            "epoch: 34, Train loss 638.328, Test loss 14.881\n",
            "epoch: 35, Train loss 657.636, Test loss 16.850\n",
            "epoch: 36, Train loss 630.457, Test loss 17.005\n",
            "epoch: 37, Train loss 626.672, Test loss 16.600\n",
            "epoch: 38, Train loss 698.653, Test loss 16.603\n",
            "epoch: 39, Train loss 627.320, Test loss 18.149\n",
            "epoch: 40, Train loss 626.402, Test loss 16.335\n",
            "epoch: 41, Train loss 636.414, Test loss 16.080\n",
            "epoch: 42, Train loss 649.653, Test loss 16.723\n",
            "epoch: 43, Train loss 635.020, Test loss 16.246\n",
            "epoch: 44, Train loss 620.894, Test loss 15.949\n",
            "epoch: 45, Train loss 713.712, Test loss 15.923\n",
            "epoch: 46, Train loss 688.629, Test loss 16.704\n",
            "epoch: 47, Train loss 626.599, Test loss 16.033\n",
            "epoch: 48, Train loss 627.272, Test loss 15.789\n",
            "epoch: 49, Train loss 628.760, Test loss 18.059\n",
            "epoch: 50, Train loss 632.871, Test loss 14.338\n",
            "epoch: 51, Train loss 636.765, Test loss 18.550\n",
            "epoch: 52, Train loss 641.560, Test loss 14.731\n",
            "epoch: 53, Train loss 622.489, Test loss 17.021\n",
            "epoch: 54, Train loss 616.696, Test loss 17.176\n",
            "epoch: 55, Train loss 612.245, Test loss 15.994\n",
            "epoch: 56, Train loss 686.893, Test loss 15.089\n",
            "epoch: 57, Train loss 744.911, Test loss 16.173\n",
            "epoch: 58, Train loss 680.266, Test loss 17.555\n",
            "epoch: 59, Train loss 619.034, Test loss 16.150\n",
            "epoch: 60, Train loss 758.671, Test loss 15.398\n",
            "epoch: 61, Train loss 683.664, Test loss 18.246\n",
            "epoch: 62, Train loss 615.415, Test loss 16.208\n",
            "epoch: 63, Train loss 622.213, Test loss 13.766\n",
            "epoch: 64, Train loss 613.641, Test loss 15.870\n",
            "epoch: 65, Train loss 610.060, Test loss 18.422\n",
            "epoch: 66, Train loss 632.659, Test loss 15.437\n",
            "epoch: 67, Train loss 735.668, Test loss 15.305\n",
            "epoch: 68, Train loss 618.083, Test loss 18.238\n",
            "epoch: 69, Train loss 613.021, Test loss 15.613\n",
            "epoch: 70, Train loss 637.114, Test loss 14.506\n",
            "epoch: 71, Train loss 804.136, Test loss 14.968\n",
            "epoch: 72, Train loss 626.984, Test loss 18.655\n",
            "epoch: 73, Train loss 694.398, Test loss 15.906\n",
            "epoch: 74, Train loss 733.449, Test loss 16.273\n",
            "epoch: 75, Train loss 611.749, Test loss 17.451\n",
            "epoch: 76, Train loss 605.025, Test loss 15.359\n",
            "epoch: 77, Train loss 684.873, Test loss 13.901\n",
            "epoch: 78, Train loss 600.235, Test loss 17.722\n",
            "epoch: 79, Train loss 613.839, Test loss 17.929\n",
            "epoch: 80, Train loss 623.057, Test loss 13.201\n",
            "epoch: 81, Train loss 725.248, Test loss 16.262\n",
            "epoch: 82, Train loss 624.388, Test loss 19.724\n",
            "epoch: 83, Train loss 702.484, Test loss 12.823\n",
            "epoch: 84, Train loss 599.036, Test loss 17.434\n",
            "epoch: 85, Train loss 604.564, Test loss 16.893\n",
            "epoch: 86, Train loss 739.915, Test loss 13.067\n",
            "epoch: 87, Train loss 601.987, Test loss 18.596\n",
            "epoch: 88, Train loss 603.297, Test loss 15.987\n",
            "epoch: 89, Train loss 632.862, Test loss 13.771\n",
            "epoch: 90, Train loss 594.907, Test loss 15.872\n",
            "epoch: 91, Train loss 617.175, Test loss 18.806\n",
            "epoch: 92, Train loss 603.233, Test loss 13.711\n",
            "epoch: 93, Train loss 667.187, Test loss 14.322\n",
            "epoch: 94, Train loss 606.370, Test loss 17.388\n",
            "epoch: 95, Train loss 610.073, Test loss 14.540\n",
            "epoch: 96, Train loss 602.489, Test loss 15.016\n",
            "epoch: 97, Train loss 600.911, Test loss 15.917\n",
            "epoch: 98, Train loss 594.170, Test loss 15.025\n",
            "epoch: 99, Train loss 727.752, Test loss 14.165\n"
          ]
        }
      ]
    }
  ]
}