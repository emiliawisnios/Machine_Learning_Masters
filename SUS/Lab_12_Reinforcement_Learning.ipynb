{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxFZ1gAEINVl"
      },
      "source": [
        "# **Some examples of Reinforcement Learning problems**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDueYn9P2See"
      },
      "source": [
        "# I. A Multi-Armed Bandit\n",
        "\n",
        "We will now look at a practical example of a Reinforcement Learning problem - the multi-armed bandit problem (one of the most popular problems in RL).\n",
        "\n",
        "> *You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.*\n",
        "\n",
        "You can think of it in analogy to a slot machine (a one-armed bandit). Each action selection is like a play of one of the slot machine’s levers, and the rewards are the payoffs for hitting the jackpot.\n",
        "\n",
        "**Solving this problem means that we can come up with an optimal policy: a strategy that allows us to select the best possible action (the one with the highest expected return) at each time step.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVps6LF03MQ8"
      },
      "source": [
        "## I. 1. Action-Value Methods\n",
        "A very simple solution is based on the action value function. Remember that an action value is the mean reward when that action is selected:\n",
        "$$ q(a)= \\mathbb{E} (R_t∣A = a)$$\n",
        "We can easily estimate q using the sample average:\n",
        "\n",
        "$$\n",
        "Q_t(a) = \\frac{\\mbox{sum of rewards when } \"a\" \\mbox{ taken prior to }\"t\"}\n",
        "{\\mbox{number of times } \"a\" \\mbox{ taken prior to } \"t\"}\n",
        "$$\n",
        "\n",
        "If we collect enough observations, our estimate gets close enough to the real function. We can then act greedily at each timestep, i.e. select the action with the highest value, to collect the highest possible rewards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQc3uCKd4jXC"
      },
      "source": [
        "## I.2.  Exploration vs. exploitation\n",
        "\n",
        "As a matter of fact, if we always act greedily as proposed in the previous paragraph, we never try out sub-optimal actions which might actually eventually lead to better results.\n",
        "\n",
        "To introduce some degree of exploration in our solution, we can use an $\\varepsilon $-greedy strategy: we select actions greedily most of the time, but every once in a while, with probability $\\varepsilon$, we select a random action, regardless of the action values.\n",
        "\n",
        "It turns out that this simple exploration method works very well, and it can significantly increase the rewards we get.\n",
        "\n",
        "One final caveat - to avoid from making our solution too computationally expensive, we compute the average incrementally according to this formula:\n",
        "$$\n",
        "Q_{n+1} = Q_n + \\frac{1}{n}[R_n−Q_n]\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4dN2voOyLLm"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Number of bandits\n",
        "k = 3\n",
        "\n",
        "# True probability of winning for each bandit\n",
        "p_bandits = [0.45, 0.40, 0.56]\n",
        "\n",
        "def pull(a):\n",
        "    \"\"\"Pull arm of bandit with index `i` and return 1 if win,\n",
        "    else return 0.\"\"\"\n",
        "    if np.random.rand() < p_bandits[a]:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKBGz3yZy5o2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33eaaf49-12f3-4372-d6b0-97a49684bc48"
      },
      "source": [
        "T = 1000\n",
        "# Our action values\n",
        "Q = [0 for _ in range(k)]\n",
        "\n",
        "# This is to keep track of the number of times we take each action\n",
        "N = [0 for _ in range(k)]\n",
        "\n",
        "eps = 0.8\n",
        "\n",
        "suma = 0\n",
        "for x in range(T):\n",
        "    if np.random.rand() > eps:\n",
        "        # Take greedy action most of the time\n",
        "        a = np.argmax(Q)\n",
        "    else:\n",
        "        # Take random action with probability eps\n",
        "        a = np.random.randint(0, k)\n",
        "\n",
        "    # Collect reward\n",
        "    reward = pull(a)\n",
        "    suma += reward\n",
        "\n",
        "    # Incremental average\n",
        "    N[a] += 1\n",
        "    Q[a] += 1/N[a] * (reward - Q[a])\n",
        "\n",
        "print(Q)\n",
        "print(suma)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.41218637992831525, 0.37956204379562025, 0.5659955257270696]\n",
            "472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz5zFwcC53zI"
      },
      "source": [
        "If we run this script for a couple of seconds, we already see that our action values are proportional to the probability of hitting the jackpots for our bandits.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sUCHOA1_4pH"
      },
      "source": [
        "# II. Self-driving Taxi problem\n",
        "Imagine we're teaching a taxi how to transport people in a parking lot to four different locations (R,G,Y,B) .\n",
        "\n",
        "You can setup up the taxi-problem environment using OpenAi’s Gym, which is one of the most used libraries for solving reinforcement problems. To install the library, use the Python package installer (pip):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOHJCF8h8Fnj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7e133dc-2b5c-4f94-c913-4feb1732941c"
      },
      "source": [
        "pip install gym"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFBTApqLIpiV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cacbe69f-2f8c-40dd-dc4f-3158235735aa"
      },
      "source": [
        "!pip install cmake 'gym[atari]' scipy"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (3.27.4.1)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.0.8)\n",
            "Collecting ale-py~=0.7.5 (from gym[atari])\n",
            "  Downloading ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[atari]) (6.0.1)\n",
            "Installing collected packages: ale-py\n",
            "Successfully installed ale-py-0.7.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj08DCfkAYTe"
      },
      "source": [
        "Now let’s see how our environment is going to render. All the models and interface for this problem are already configured in Gym and named under Taxi-V3. Use the code snippet below to render this environment:\n",
        "\n",
        "> <font color='blue'> “There are 4 locations (labelled by different letters), and our job is to pick up the passenger at one location and drop him off at another. We receive +20 points for a successful drop-off and lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.”</font> (Source: https://gym.openai.com/envs/Taxi-v3/ )\n",
        "\n",
        "This will be the rendered output on your console:\n",
        "\n",
        "\n",
        "    \"+---------+\",\n",
        "    \"|R: | : :G|\",\n",
        "    \"| : | : : |\",\n",
        "    \"| : : : : |\",\n",
        "    \"| | : | : |\",\n",
        "    \"|Y| : |B: |\",\n",
        "    \"+---------+\",\n",
        "\n",
        "\n",
        "\n",
        "Env is the core of OpenAi Gym, which is the unified environment interface. The following are the env methods that will be quite helpful to us:\n",
        "\n",
        "1.   **<font color='red'>env.reset</font>**: Resets the\n",
        "environment and returns a random initial state.\n",
        "\n",
        "2.  **<font color='red'> env.step(action)</font>**: Step the environment by one timestep. It returns the following variables:\n",
        "\n",
        "> * <font color='red'> observation</font>: Observations of the environment.\n",
        "* <font color='red'> reward</font>: If your action was beneficial or not.\n",
        "* <font color='red'> done</font>: Indicates if we have successfully picked up and dropped off a passenger, also called one episode.\n",
        "* <font color='red'> info</font>: Additional info such as performance and latency for debugging purposes.\n",
        "\n",
        "3. **<font color='red'> env.render</font>**: Renders one frame of the environment (helpful in visualizing the environment).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fGVEwqd_ts3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e55851ec-ac75-4589-b5c5-a4fd2051fda4"
      },
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make(\"Taxi-v3\").env\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdJMGmNRNz2G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb87f63a-6eac-4f25-ad13-dd5ad64780ba"
      },
      "source": [
        "env.reset() # reset environment to a new, random state\n",
        "env.render()\n",
        "\n",
        "print(\"Action Space {}\".format(env.action_space))\n",
        "print(\"State Space {}\".format(env.observation_space))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space Discrete(6)\n",
            "State Space Discrete(500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1U3exbVPcfU"
      },
      "source": [
        "## II.1. State Space\n",
        "\n",
        "\n",
        "Lets dive deeper in breaking down the problem. The taxi is the only car in this parking lot. We can break the parking lot into a 5x5 grid, which gives us 25 possible taxi locations. These 25 locations are one part of our state space. Notice the current location state of our taxi is coordinate (3, 1).\n",
        "\n",
        "![axi-v2 Env](https://storage.googleapis.com/lds-media/images/Reinforcement_Learning_Taxi_Env.width-1200.png)\n",
        "\n",
        "In the environment, there are four possible locations where you can drop off the passengers: R, G, Y, B or [(0,0), (0,4), (4,0), (4,3)] in (row, col) coordinates if you can interpret the above-rendered environment as a coordinate axis.\n",
        "\n",
        "When we also account for one (1) additional passenger state of being inside the taxi, we can take all combinations of passenger locations and destination locations to come to a total number of states for our taxi environment — there are four (4) destinations and five (4 + 1) passenger locations. So, our taxi environment has 5×5×5×4=500 total possible states.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzcF05hspcHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f31c184-ee6d-46bc-9cdb-174f29e41189"
      },
      "source": [
        "state = env.encode(3, 1, 2, 0)\n",
        "# (taxi row, taxi column, passenger index, destination index)\n",
        "print(\"State:\", state)\n",
        "\n",
        "env.s = state\n",
        "env.render()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: 328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8onwjN72PuTr"
      },
      "source": [
        "## II.2 Action Space\n",
        "\n",
        "The agent encounters one of the 500 states, and it takes action. The action in our case can be to move in a direction or decide to pick up/drop off a passenger.\n",
        "\n",
        "In other words, we have six possible actions:  **<font color='red'>pickup, drop, north, east,south, west</font>**\n",
        "(These four directions are the moves by which the taxi is moved.)\n",
        "\n",
        "This is the action space: the set of all the actions that our agent can take in a given state.\n",
        "\n",
        "You’ll notice in the illustration above, that the taxi cannot perform certain actions in certain states due to walls. In the environment’s code, we will simply provide a -1 penalty for every wall hit and the taxi won’t move anywhere. This will just rack up penalties causing the taxi to consider going around the wall.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNd8HhhAQifb"
      },
      "source": [
        "## II.3 Reward Table:\n",
        "\n",
        "When the taxi environment is created, there is an initial reward table that’s also created, called P. We can think of it like a matrix that has the number of states as rows and number of actions as columns, i.e. states × actions matrix.\n",
        "\n",
        "Since every state is in this matrix, we can see the default reward values assigned to our illustration’s state:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lzHfiXhOMgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c537da1-8f03-4443-9eb3-f63d0715a6c5"
      },
      "source": [
        "env.P[328]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 428, -1, False)],\n",
              " 1: [(1.0, 228, -1, False)],\n",
              " 2: [(1.0, 348, -1, False)],\n",
              " 3: [(1.0, 328, -1, False)],\n",
              " 4: [(1.0, 328, -10, False)],\n",
              " 5: [(1.0, 328, -10, False)]}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rj7p9iYJaOz"
      },
      "source": [
        "This dictionary has a structure\n",
        "\n",
        "<font color='green'>{action: [(probability, nextstate, reward, done)]}.</font>\n",
        "\n",
        "> * The 0–5 corresponds to the actions (south, north, east, west, pickup, drop off) the taxi can perform at our current state in the illustration.\n",
        "  - 0 = south\n",
        "  - 1 = north\n",
        "  - 2 = east\n",
        "  - 3 = west\n",
        "  - 4 = pickup\n",
        "  - 5 = dropof\n",
        "* done is used to tell us when we have successfully dropped off a passenger in the right location.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XD206DXp3nI"
      },
      "source": [
        "## II.4. Solution without RL\n",
        "\n",
        "Let's see what would happen if we try to brute-force our way to solving the problem without RL.\n",
        "\n",
        "Since we have our P table for default rewards in each state, we can try to have our taxi navigate just using that.\n",
        "\n",
        "We'll create an infinite loop which runs until one passenger reaches one destination (one episode), or in other words, when the received reward is 20. The env.action_space.sample() method automatically selects one random action from set of all possible actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn9J73Ymp2ep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81065d51-736f-4f9b-f58e-f390575aab33"
      },
      "source": [
        "# set environment to illustration's state\n",
        "env.s = 328\n",
        "\n",
        "# Setting the number of iterations, penalties and reward to zero,\n",
        "epochs = 0\n",
        "penalties, reward = 0, 0\n",
        "\n",
        "# for animation\n",
        "frames = []\n",
        "\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    state, reward, done, info = env.step(action)\n",
        "\n",
        "    if reward == -10:\n",
        "        penalties += 1\n",
        "\n",
        "    # Put each rendered frame into dict for animation\n",
        "    frames.append({\n",
        "        'frame': env.render(mode='ansi'),\n",
        "        'episode': 0,\n",
        "        'state': state,\n",
        "        'action': action,\n",
        "        'reward': reward\n",
        "        }\n",
        "    )\n",
        "\n",
        "    epochs += 1\n",
        "\n",
        "\n",
        "print(\"Timesteps taken: {}\".format(epochs))\n",
        "print(\"Penalties incurred: {}\".format(penalties))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timesteps taken: 7382\n",
            "Penalties incurred: 2414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHE8P6oWvktL"
      },
      "source": [
        "### Printing frames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJUSBfKgqG-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11dbb0ad-fd48-4f56-cfda-00e623ee410c"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Episode: {frame['episode']}\")\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.1)\n",
        "\n",
        "print_frames(frames)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Episode: 0\n",
            "Timestep: 7382\n",
            "State: 410\n",
            "Action: 5\n",
            "Reward: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66mkc4Vxy1VO"
      },
      "source": [
        "Not good. Our taxi takes thousands of timesteps and makes lots of wrong drop offs to deliver just one passenger to the right destination.\n",
        "\n",
        "This is because we aren't learning from past experience. We can run this over and over, and it will never optimize. The agent has no memory of which action was best for each state, which is exactly what Reinforcement Learning will do for us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvTRK5y9vzq8"
      },
      "source": [
        "## II.5. Reinforcement Learning using Q-Learning\n",
        "\n",
        "Essentially, Q-learning lets the agent use the environment's rewards to learn, over time, the best action to take in a given state.\n",
        "\n",
        "In our Taxi environment, we have the reward table, P, that the agent will learn from. It does thing by looking receiving a reward for taking an action in the current state, then updating a Q-value to remember if that action was beneficial.\n",
        "\n",
        "The values store in the Q-table are called a Q-values, and they map to a (state, action) combination.\n",
        "\n",
        "A Q-value for a particular state-action combination is representative of the \"quality\" of an action taken from that state. Better Q-values imply better chances of getting greater rewards.\n",
        "\n",
        "For example, if the taxi is faced with a state that includes a passenger at its current location, it is highly likely that the Q-value for pickup is higher when compared to other actions, like dropoff or north.\n",
        "\n",
        "Q-values are initialized to an arbitrary value, and as the agent exposes itself to the environment and receives different rewards by executing different actions, the Q-values are updated using the equation:\n",
        "$$\n",
        "Q({\\small state}, {\\small action}) \\leftarrow (1 - \\alpha) Q({\\small state}, {\\small action}) + \\alpha \\Big({\\small reward} + \\gamma \\max_{a} Q({\\small next \\ state}, {\\small all \\ actions})\\Big)\n",
        "$$\n",
        "Where:\n",
        "\n",
        "- $\\alpha$ (alpha) is the learning rate ($0< \\alpha \\leq 1$) - Just like in supervised learning settings, $\\alpha$  is the extent to which our Q-values are being updated in every iteration.\n",
        "\n",
        "- $\\gamma$ (gamma) is the discount factor ($0\\leq \\gamma \\leq 1$) - determines how much importance we want to give to future rewards. A high value for the discount factor (close to 1) captures the long-term effective award, whereas, a discount factor of 0 makes our agent consider only immediate reward, hence making it greedy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlBaUosnv2AB"
      },
      "source": [
        "import numpy as np\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPw-UQi1z6Dd"
      },
      "source": [
        "**Q-Table**\n",
        "\n",
        "The Q-table is a matrix where we have a row for every state (500) and a column for every action (6). It's first initialized to 0, and then values are updated after training. Note that the Q-table has the same dimensions as the reward table, but it has a completely different purpose.\n",
        "\n",
        "![alt text](https://storage.googleapis.com/lds-media/images/q-matrix-initialized-to-learned_gQq0BFs.width-1200.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJSiawKOPNrz"
      },
      "source": [
        "### Below is the algorithm in brief:\n",
        "\n",
        "* Step 1: Initialize the Q-table with all zeros and Q-values to arbitrary constants.\n",
        "* Step 2: Let the agent react to the environment and explore the actions. For each change in state, select any one among all possible actions for the current state (S).\n",
        "* Step 3: Travel to the next state (S’) as a result of that action (a).\n",
        "* Step 4: For all possible actions from the state (S’) select the one with the highest Q-value.\n",
        "* Step 5: Update Q-table values using the equation.\n",
        "* State 6: Change the next state as the current state.\n",
        "* Step 7: If goal state is reached, then end and repeat the process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HleOpP7Uv_kh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e002409-41d6-4475-ef6d-2f216e8d57f1"
      },
      "source": [
        "%%time\n",
        "\"\"\"Training the agent\"\"\"\n",
        "\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "epsilon = 0.1\n",
        "\n",
        "# For plotting metrics\n",
        "all_epochs = []\n",
        "all_penalties = []\n",
        "\n",
        "for i in range(1, 100001):\n",
        "    state = env.reset()\n",
        "\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample() # Explore action space\n",
        "        else:\n",
        "            action = np.argmax(q_table[state]) # Exploit learned values\n",
        "\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        old_value = q_table[state, action]\n",
        "        next_max = np.max(q_table[next_state])\n",
        "\n",
        "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "        q_table[state, action] = new_value\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episode: {i}\")\n",
        "\n",
        "print(\"Training finished.\\n\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n",
            "CPU times: user 1min 20s, sys: 6.92 s, total: 1min 27s\n",
            "Wall time: 1min 26s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLbEFB_nwGFw"
      },
      "source": [
        "Check the q_table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6ez8j2mwKPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9561f2e5-e3bb-479e-c109-e1951acdaa65"
      },
      "source": [
        "q_table[382]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.46770886, -2.4510224 , -2.46413162, -2.4510224 , -9.81996992,\n",
              "       -9.67518131])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PN7RZ42wL6J"
      },
      "source": [
        "### Evaluate smart taxi performance after Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJ0B9BpkwUQf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f410aa1-832d-46c2-900f-390eb19ee9ae"
      },
      "source": [
        "total_epochs, total_penalties = 0, 0\n",
        "episodes = 100\n",
        "frames = []\n",
        "\n",
        "for ep in range(episodes):\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state])\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        # Put each rendered frame into dict for animation\n",
        "        frames.append({\n",
        "            'frame': env.render(mode='ansi'),\n",
        "            'episode': ep,\n",
        "            'state': state,\n",
        "            'action': action,\n",
        "            'reward': reward\n",
        "            }\n",
        "        )\n",
        "        epochs += 1\n",
        "\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 13.77\n",
            "Average penalties per episode: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDK7ariswihl"
      },
      "source": [
        "### Visualization of Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfccG8mFwrHw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e92b591-64c6-478a-878a-31b010a240fc"
      },
      "source": [
        "print_frames(frames)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Episode: 99\n",
            "Timestep: 1377\n",
            "State: 475\n",
            "Action: 5\n",
            "Reward: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N_gDrTX-JUV"
      },
      "source": [
        "# Acknowledgment\n",
        "The materials were prepared on the base of following materials:\n",
        "\n",
        "\n",
        "\n",
        "https://builtin.com/data-science/reinforcement-learning-python\n",
        "\n",
        "https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
        "\n",
        "\n",
        "https://www.kaggle.com/karthikcs1/reinforcement-learning-taxi-v3-openai\n",
        "\n",
        "https://stackabuse.com/introduction-to-reinforcement-learning-with-python/\n",
        "\n",
        "https://github.com/casey-barr/open-ai-taxi-problem/blob/master/open_ai_taxi_problem.ipynb"
      ]
    }
  ]
}