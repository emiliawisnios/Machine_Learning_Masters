{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kopia notatnika GSN/DNN 2021/22 Lab5 - Batchnorm and Convnets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcTwzhX8fBqs"
      },
      "source": [
        "Code based on https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
        "\n",
        "This exercise covers two aspects:\n",
        "* In tasks 1-6 you will implement mechanisms that allow training deeper models (better initialization, batch normalization). Note that for dropout and batch norm you are expected to implement it yourself without relying on ready-made components from Pytorch.\n",
        "* In task 7 you will implement a convnet using [conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html).\n",
        "\n",
        "\n",
        "Tasks:\n",
        "1. Check that the given implementation reaches 95% test accuracy for\n",
        "   architecture input-64-64-10 in a few thousand batches.\n",
        "2. Improve initialization and check that the network learns much faster\n",
        "   and reaches over 97% test accuracy. A good basic initialization scheme is so-called Glorot initialization. For a set of weights going from a layer with $n_{in}$ neurons to a layer with $n_{out}$ neurons, it samples each weight from normal distribution with $0$ mean and standard deviation of $\\sqrt{\\frac{2}{n_{in}+n_{out}}}$.\n",
        "3. Check, that with proper initialization we can train architecture\n",
        "   input-64-64-64-64-64-10, while with bad initialization it does\n",
        "   not even get off the ground.\n",
        "4. Add dropout implemented in pytorch\n",
        "5. Check that with 10 hidden layers (64 units each) even with proper\n",
        "    initialization the network has a hard time to start learning.\n",
        "6. Implement batch normalization (use train mode also for testing - it should perform well enough):\n",
        "    * compute batch mean and variance\n",
        "    * add new variables beta and gamma\n",
        "    * check that the networks learns much faster for 5 layers\n",
        "    * check that the network learns even for 10 hidden layers.\n",
        "7. So far we worked with a fully connected network. Design and implement in pytorch (by using pytorch functions) a simple convolutional network and achieve 99% test accuracy. The architecture is up to you, but even a few convolutional layers should be enough."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYAsziKffBFV"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMtap4QCfBH8"
      },
      "source": [
        "class Linear(torch.nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(Linear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.bias = Parameter(torch.Tensor(out_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        #self.weight.data.normal_(mean=0,std=0.25)\n",
        "        init.xavier_normal_(self.weight)\n",
        "        init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        r = x.matmul(self.weight.t())\n",
        "        r += self.bias\n",
        "        return r"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9JctYsFFtmW"
      },
      "source": [
        "def batch_normalization(x, gamma, beta, moving_mean, moving_var, eps=1e-5, momentum=0.9):\n",
        "    if not torch.is_grad_enabled():\n",
        "        x_norm = (x - moving_mean) / torch.sqrt(moving_var + eps)\n",
        "    else:\n",
        "        mean = x.mean(dim=0, keepdims=True)\n",
        "        var = ((x - mean) ** 2).mean(dim=0, keepdims=True)\n",
        "        x_norm = (x - mean) / torch.sqrt(var + eps)\n",
        "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
        "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
        "    x = gamma * x_norm + beta\n",
        "    return x, moving_mean.data, moving_var.data\n",
        "\n",
        "\n",
        "class BatchNorm(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.beta = nn.Parameter(torch.zeros(1,64))\n",
        "    self.gamma = nn.Parameter(torch.ones(1,64))\n",
        "    self.moving_mean = torch.zeros(1,64)\n",
        "    self.moving_var = torch.ones(1,64)\n",
        " \n",
        "  def forward(self, X, eps=1e-5, momentum=0.9):\n",
        "    if self.moving_mean.device != X.device:\n",
        "      self.moving_mean = self.moving_mean.to(X.device)\n",
        "      self.moving_var = self.moving_var.to(X.device)\n",
        "    X, self.moving_mean, self.moving_var = batch_normalization(X, self.gamma, self.beta, \n",
        "                                                                   self.moving_mean, self.moving_var)\n",
        "    return X\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBJxmGUUFWc4"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, num_hidden_layers=4, dropout_prob=0.1, use_dropout=False):\n",
        "        super(Net, self).__init__()\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.dropout_prob = dropout_prob\n",
        "        self.use_dropout = use_dropout\n",
        "\n",
        "\n",
        "        self.input_layer = Linear(784, 64)\n",
        "        self.hidden_layers = [Linear(64, 64) for _ in range(self.num_hidden_layers)]\n",
        "        self.output_layer = Linear(64, 10)\n",
        "        self.dropout = nn.Dropout(self.dropout_prob)\n",
        "        self.batch_norm = [BatchNorm() for _ in range(self.num_hidden_layers)]\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = F.relu(self.input_layer(x))\n",
        "\n",
        "        for i in range(self.num_hidden_layers):\n",
        "            x = self.batch_norm[i](self.hidden_layers[i](x))\n",
        "            x = F.relu(x)\n",
        "            if self.use_dropout:\n",
        "              x = self.dropout(x)\n",
        "\n",
        "        return self.output_layer(x)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgfUP23AfBMd"
      },
      "source": [
        "class MnistTrainer(object):\n",
        "    def __init__(self, batch_size):\n",
        "        transform = transforms.Compose(\n",
        "                [transforms.ToTensor()])\n",
        "        self.trainset = torchvision.datasets.MNIST(\n",
        "            root='./data',\n",
        "            download=True,\n",
        "            train=True,\n",
        "            transform=transform)\n",
        "        self.trainloader = torch.utils.data.DataLoader(\n",
        "            self.trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "        self.testset = torchvision.datasets.MNIST(\n",
        "            root='./data',\n",
        "            train=False,\n",
        "            download=True, transform=transform)\n",
        "        self.testloader = torch.utils.data.DataLoader(\n",
        "            self.testset, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "    def train(self):\n",
        "        net = Net()\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
        "\n",
        "        for epoch in range(20):\n",
        "            running_loss = 0.0\n",
        "            for i, data in enumerate(self.trainloader, 0):\n",
        "                inputs, labels = data\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = net(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                if i % 100 == 99:\n",
        "                    print('[%d, %5d] loss: %.3f' %\n",
        "                          (epoch + 1, i + 1, running_loss / 100))\n",
        "                    running_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            with torch.no_grad():\n",
        "                for data in self.testloader:\n",
        "                    images, labels = data\n",
        "                    outputs = net(images)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "            print('Accuracy of the network on the {} test images: {} %'.format(\n",
        "                total, 100 * correct / total))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezvIQbgsfBRT",
        "outputId": "15d04d17-628b-4420-bb57-6abae3d9d1d4"
      },
      "source": [
        "trainer = MnistTrainer(batch_size=128)\n",
        "trainer.train()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   100] loss: 0.699\n",
            "[1,   200] loss: 0.349\n",
            "[1,   300] loss: 0.304\n",
            "[1,   400] loss: 0.277\n",
            "Accuracy of the network on the 10000 test images: 93.21 %\n",
            "[2,   100] loss: 0.225\n",
            "[2,   200] loss: 0.213\n",
            "[2,   300] loss: 0.224\n",
            "[2,   400] loss: 0.210\n",
            "Accuracy of the network on the 10000 test images: 94.47 %\n",
            "[3,   100] loss: 0.185\n",
            "[3,   200] loss: 0.177\n",
            "[3,   300] loss: 0.194\n",
            "[3,   400] loss: 0.174\n",
            "Accuracy of the network on the 10000 test images: 95.01 %\n",
            "[4,   100] loss: 0.156\n",
            "[4,   200] loss: 0.160\n",
            "[4,   300] loss: 0.157\n",
            "[4,   400] loss: 0.173\n",
            "Accuracy of the network on the 10000 test images: 94.79 %\n",
            "[5,   100] loss: 0.154\n",
            "[5,   200] loss: 0.150\n",
            "[5,   300] loss: 0.133\n",
            "[5,   400] loss: 0.152\n",
            "Accuracy of the network on the 10000 test images: 95.39 %\n",
            "[6,   100] loss: 0.125\n",
            "[6,   200] loss: 0.128\n",
            "[6,   300] loss: 0.132\n",
            "[6,   400] loss: 0.143\n",
            "Accuracy of the network on the 10000 test images: 95.49 %\n",
            "[7,   100] loss: 0.118\n",
            "[7,   200] loss: 0.124\n",
            "[7,   300] loss: 0.125\n",
            "[7,   400] loss: 0.135\n",
            "Accuracy of the network on the 10000 test images: 95.49 %\n",
            "[8,   100] loss: 0.110\n",
            "[8,   200] loss: 0.119\n",
            "[8,   300] loss: 0.120\n",
            "[8,   400] loss: 0.116\n",
            "Accuracy of the network on the 10000 test images: 95.62 %\n",
            "[9,   100] loss: 0.108\n",
            "[9,   200] loss: 0.106\n",
            "[9,   300] loss: 0.111\n",
            "[9,   400] loss: 0.113\n",
            "Accuracy of the network on the 10000 test images: 95.48 %\n",
            "[10,   100] loss: 0.104\n",
            "[10,   200] loss: 0.103\n",
            "[10,   300] loss: 0.105\n",
            "[10,   400] loss: 0.110\n",
            "Accuracy of the network on the 10000 test images: 95.87 %\n",
            "[11,   100] loss: 0.093\n",
            "[11,   200] loss: 0.097\n",
            "[11,   300] loss: 0.102\n",
            "[11,   400] loss: 0.102\n",
            "Accuracy of the network on the 10000 test images: 95.75 %\n",
            "[12,   100] loss: 0.084\n",
            "[12,   200] loss: 0.091\n",
            "[12,   300] loss: 0.090\n",
            "[12,   400] loss: 0.099\n",
            "Accuracy of the network on the 10000 test images: 95.67 %\n",
            "[13,   100] loss: 0.080\n",
            "[13,   200] loss: 0.091\n",
            "[13,   300] loss: 0.088\n",
            "[13,   400] loss: 0.093\n",
            "Accuracy of the network on the 10000 test images: 95.73 %\n",
            "[14,   100] loss: 0.076\n",
            "[14,   200] loss: 0.086\n",
            "[14,   300] loss: 0.084\n",
            "[14,   400] loss: 0.093\n",
            "Accuracy of the network on the 10000 test images: 95.77 %\n",
            "[15,   100] loss: 0.077\n",
            "[15,   200] loss: 0.083\n",
            "[15,   300] loss: 0.079\n",
            "[15,   400] loss: 0.087\n",
            "Accuracy of the network on the 10000 test images: 95.78 %\n",
            "[16,   100] loss: 0.069\n",
            "[16,   200] loss: 0.078\n",
            "[16,   300] loss: 0.087\n",
            "[16,   400] loss: 0.091\n",
            "Accuracy of the network on the 10000 test images: 95.85 %\n",
            "[17,   100] loss: 0.073\n",
            "[17,   200] loss: 0.080\n",
            "[17,   300] loss: 0.077\n",
            "[17,   400] loss: 0.076\n",
            "Accuracy of the network on the 10000 test images: 96.12 %\n",
            "[18,   100] loss: 0.070\n",
            "[18,   200] loss: 0.072\n",
            "[18,   300] loss: 0.075\n",
            "[18,   400] loss: 0.081\n",
            "Accuracy of the network on the 10000 test images: 95.88 %\n",
            "[19,   100] loss: 0.070\n",
            "[19,   200] loss: 0.063\n",
            "[19,   300] loss: 0.069\n",
            "[19,   400] loss: 0.080\n",
            "Accuracy of the network on the 10000 test images: 95.7 %\n",
            "[20,   100] loss: 0.063\n",
            "[20,   200] loss: 0.070\n",
            "[20,   300] loss: 0.076\n",
            "[20,   400] loss: 0.064\n",
            "Accuracy of the network on the 10000 test images: 95.93 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpK5JaXdR2FQ"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 5, 1, 2)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 5, 1, 2)\n",
        "        self.fc1 = nn.Linear(32 * 7 *7, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        return x"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyt6TJJfSW0A"
      },
      "source": [
        "class ConvMnistTrainer(object):\n",
        "    def __init__(self, batch_size):\n",
        "        transform = transforms.Compose(\n",
        "                [transforms.ToTensor()])\n",
        "        self.trainset = torchvision.datasets.MNIST(\n",
        "            root='./data',\n",
        "            download=True,\n",
        "            train=True,\n",
        "            transform=transform)\n",
        "        self.trainloader = torch.utils.data.DataLoader(\n",
        "            self.trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "        self.testset = torchvision.datasets.MNIST(\n",
        "            root='./data',\n",
        "            train=False,\n",
        "            download=True, transform=transform)\n",
        "        self.testloader = torch.utils.data.DataLoader(\n",
        "            self.testset, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "    def train(self):\n",
        "        net = ConvNet()\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
        "\n",
        "        for epoch in range(20):\n",
        "            running_loss = 0.0\n",
        "            for i, data in enumerate(self.trainloader, 0):\n",
        "                inputs, labels = data\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = net(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                if i % 100 == 99:\n",
        "                    print('[%d, %5d] loss: %.3f' %\n",
        "                          (epoch + 1, i + 1, running_loss / 100))\n",
        "                    running_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            with torch.no_grad():\n",
        "                for data in self.testloader:\n",
        "                    images, labels = data\n",
        "                    outputs = net(images)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "            print('Accuracy of the network on the {} test images: {} %'.format(\n",
        "                total, 100 * correct / total))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egX2T-Q9Seag",
        "outputId": "3536ee5b-aafe-47c4-cdc7-1a7ec22419b0"
      },
      "source": [
        "trainer = ConvMnistTrainer(batch_size=128)\n",
        "trainer.train()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   100] loss: 0.524\n",
            "[1,   200] loss: 0.110\n",
            "[1,   300] loss: 0.091\n",
            "[1,   400] loss: 0.071\n",
            "Accuracy of the network on the 10000 test images: 98.37 %\n",
            "[2,   100] loss: 0.043\n",
            "[2,   200] loss: 0.052\n",
            "[2,   300] loss: 0.048\n",
            "[2,   400] loss: 0.047\n",
            "Accuracy of the network on the 10000 test images: 98.66 %\n",
            "[3,   100] loss: 0.032\n",
            "[3,   200] loss: 0.041\n",
            "[3,   300] loss: 0.035\n",
            "[3,   400] loss: 0.036\n",
            "Accuracy of the network on the 10000 test images: 98.66 %\n",
            "[4,   100] loss: 0.031\n",
            "[4,   200] loss: 0.023\n",
            "[4,   300] loss: 0.029\n",
            "[4,   400] loss: 0.033\n",
            "Accuracy of the network on the 10000 test images: 98.67 %\n",
            "[5,   100] loss: 0.022\n",
            "[5,   200] loss: 0.022\n",
            "[5,   300] loss: 0.026\n",
            "[5,   400] loss: 0.024\n",
            "Accuracy of the network on the 10000 test images: 99.04 %\n",
            "[6,   100] loss: 0.016\n",
            "[6,   200] loss: 0.020\n",
            "[6,   300] loss: 0.018\n",
            "[6,   400] loss: 0.025\n",
            "Accuracy of the network on the 10000 test images: 98.76 %\n",
            "[7,   100] loss: 0.015\n",
            "[7,   200] loss: 0.014\n",
            "[7,   300] loss: 0.024\n",
            "[7,   400] loss: 0.017\n",
            "Accuracy of the network on the 10000 test images: 98.94 %\n",
            "[8,   100] loss: 0.014\n",
            "[8,   200] loss: 0.015\n",
            "[8,   300] loss: 0.016\n",
            "[8,   400] loss: 0.014\n",
            "Accuracy of the network on the 10000 test images: 98.88 %\n",
            "[9,   100] loss: 0.015\n",
            "[9,   200] loss: 0.014\n",
            "[9,   300] loss: 0.014\n",
            "[9,   400] loss: 0.010\n",
            "Accuracy of the network on the 10000 test images: 98.98 %\n",
            "[10,   100] loss: 0.007\n",
            "[10,   200] loss: 0.014\n",
            "[10,   300] loss: 0.012\n",
            "[10,   400] loss: 0.012\n",
            "Accuracy of the network on the 10000 test images: 99.1 %\n",
            "[11,   100] loss: 0.009\n",
            "[11,   200] loss: 0.008\n",
            "[11,   300] loss: 0.012\n",
            "[11,   400] loss: 0.009\n",
            "Accuracy of the network on the 10000 test images: 99.08 %\n",
            "[12,   100] loss: 0.007\n",
            "[12,   200] loss: 0.004\n",
            "[12,   300] loss: 0.007\n",
            "[12,   400] loss: 0.015\n",
            "Accuracy of the network on the 10000 test images: 98.88 %\n",
            "[13,   100] loss: 0.007\n",
            "[13,   200] loss: 0.004\n",
            "[13,   300] loss: 0.009\n",
            "[13,   400] loss: 0.007\n",
            "Accuracy of the network on the 10000 test images: 98.98 %\n",
            "[14,   100] loss: 0.003\n",
            "[14,   200] loss: 0.004\n",
            "[14,   300] loss: 0.007\n",
            "[14,   400] loss: 0.006\n",
            "Accuracy of the network on the 10000 test images: 99.23 %\n",
            "[15,   100] loss: 0.006\n",
            "[15,   200] loss: 0.006\n",
            "[15,   300] loss: 0.006\n",
            "[15,   400] loss: 0.006\n",
            "Accuracy of the network on the 10000 test images: 98.98 %\n",
            "[16,   100] loss: 0.006\n",
            "[16,   200] loss: 0.005\n",
            "[16,   300] loss: 0.009\n",
            "[16,   400] loss: 0.006\n",
            "Accuracy of the network on the 10000 test images: 98.96 %\n",
            "[17,   100] loss: 0.003\n",
            "[17,   200] loss: 0.004\n",
            "[17,   300] loss: 0.003\n",
            "[17,   400] loss: 0.002\n",
            "Accuracy of the network on the 10000 test images: 99.15 %\n",
            "[18,   100] loss: 0.001\n",
            "[18,   200] loss: 0.001\n",
            "[18,   300] loss: 0.002\n",
            "[18,   400] loss: 0.003\n",
            "Accuracy of the network on the 10000 test images: 99.23 %\n",
            "[19,   100] loss: 0.002\n",
            "[19,   200] loss: 0.002\n",
            "[19,   300] loss: 0.001\n",
            "[19,   400] loss: 0.002\n",
            "Accuracy of the network on the 10000 test images: 99.06 %\n",
            "[20,   100] loss: 0.002\n",
            "[20,   200] loss: 0.001\n",
            "[20,   300] loss: 0.001\n",
            "[20,   400] loss: 0.000\n",
            "Accuracy of the network on the 10000 test images: 99.23 %\n"
          ]
        }
      ]
    }
  ]
}